Attribute
output of one resource as input to another

resource "local_file" "pet" {
filename = var.filename
content = "my favourite pet is ${random_pet.my-pet.id}"
}

resource "random_pet" "my-pet" {
prefix = var.prefix
separator= var.separator
length= var.length
}


===================life cycle rules=============
by default, destroy old file and create

but if we want to create new file before destrying existing then use lifecycle rule,
 inside resource block,


lifecycle{
create_before_destroy=true
}


----------------
lifecycle{
prevent_destroy = true
}


------------
lifecycle{
ignore_changes = [
 tags
]
}

=====================data source=================
a file on which terraform does not have control of

only reads infrastructure, do not create, destroy, update

data "local_file" "dog"{
filename = "/root/dog.txt"
}



=========================================

count meta-argument to loop through the project-sapphire-users variable and create all the users in the list.


You may want to make use of the length function to get the length of the list.


sol:

resource "aws_iam_user" "users" {
     name = var.project-sapphire-users[count.index]
     count = length(var.project-sapphire-users)
}

first length will count then , this length will be used in name 


================================================Dynamo DB==========================================================
resource "aws_dynamodb_table" "project_sapphire_inventory" {
  name           = "inventory"
  billing_mode   = "PAY_PER_REQUEST"
  hash_key       = "AssetID"

  attribute {
    name = "AssetID"
    type = "N"
  }
  attribute {
    name = "AssetName"
    type = "S"
  }
  attribute {
    name = "age"
    type = "N"
  }
  attribute {
    name = "Hardware"
    type = "B"
  }
  global_secondary_index {
    name             = "AssetName"
    hash_key         = "AssetName"
    projection_type    = "ALL"

  }
  global_secondary_index {
    name             = "age"
    hash_key         = "age"
    projection_type    = "ALL"

  }
  global_secondary_index {
    name             = "Hardware"
    hash_key         = "Hardware"
    projection_type    = "ALL"

  }
}
resource "aws_dynamodb_table_item" "upload" {
  table_name = aws_dynamodb_table.project_sapphire_inventory.name
  hash_key   = aws_dynamodb_table.project_sapphire_inventory.hash_key
  item = <<EOF
 {
  "AssetID": {"N": "1"},
  "AssetName": {"S": "printer"},
  "age": {"N": "5"},
  "Hardware": {"B": "true" }
}
EOF
}


====================terraform s3 as backend=====

terraform {
  backend "s3" {
    key = "terraform.tfstate"
    region = "us-east-1"
    bucket = "remote-state"
}
}

=================================EC2============================

resource "aws_instance" "cerberus" {
  ami           = var.ami
  instance_type = var.instance_type
  user_data     = file("./install-nginx.sh")

}
resource "aws_key_pair" "cerberus-key" {
  key_name   = "cerberus"
  public_key = file(".ssh/cerberus.pub")
}
resource "aws_eip" "eip" {
  vpc      = true
  instance = aws_instance.cerberus.id
  provisioner "local-exec" {
    command = "echo ${aws_eip.eip.public_dns} >> /root/cerberus_public_dns.txt"
  }

}
variable "ami" {
  default = "ami-06178cf087598769c"
}
variable "instance_type" {
  default = "m5.large"

}
variable "region" {
  default = "eu-west-2"
}
-------
terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "4.15.0"
    }
  }
}
provider "aws" {
  region                      = var.region
  skip_credentials_validation = true
  skip_requesting_account_id  = true

  endpoints {
    ec2 = "http://aws:4566"
  }
}
=========install-nginx.sh==========
#!/bin/bash
sudo yum update -y
sudo yum install nginx -y
sudo systemctl start nginx



===========================Terraform provisioners====
Terraform provisioners have nothing in common with providers, they allow the execution of various commands or scripts on either local or remote machines, and they can also transfer files from a local environment to a remote one.
There are three available provisioners: file (used for copying), local-exec (used for local operations), remote-exec (used for remote operations). The file and remote-exec provisioners need a connection block to be able to do the remote operations.

